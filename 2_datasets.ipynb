{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Manejo de Datos en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import functools\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing import preprocessing\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase Dataset\n",
    "\n",
    "La clase abstracta [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) es la clase base para construir un dataset de PyTorch. Cualquier dataset personalizado debe heredar de dicha clase e implementar los siguientes métodos:\n",
    "\n",
    "- `__len__`: Para que `len(dataset)` devuelva el tamaño del conjunto de datos.\n",
    "- `__getitem__`: Para soportar indexado de manera que `dataset[i]` devuelva el elemento `i`. Es común que en ciertos casos se utilice este método para levantar el dato real (e.g. una imagen) mientras que lo que se guarde en el dataset sea sólo una referencia a dicho dato (e.g. un path a la imagen). De esta manera se evita cargar muchas imágenes en memoria, haciendo que sea menos demandante a nivel RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 50000 elements\n",
      "Sample element:\n",
      "{'data': \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\", 'target': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.dataset = pd.read_csv(path)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.tolist()  # Deal with list of items instead of tensor\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.iloc[item][\"review\"],\n",
    "            \"target\": self.dataset.iloc[item][\"sentiment\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item\n",
    "\n",
    "dataset = IMDBReviewsDataset(\"./data/imdb_reviews.csv.gz\")\n",
    "print(f\"Dataset loaded with {len(dataset)} elements\")\n",
    "print(f\"Sample element:\\n{dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformaciones\n",
    "\n",
    "El ejemplo anterior nos muestra el uso básico, pero claramente no podemos pasarle eso a una red neuronal, no puede manejar texto. Es para eso que tenemos que hacer algún tipo de transformación sobre los atributos (en este caso el único atributo es el texto). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización\n",
    "\n",
    "En particular, como vemos en el caso anterior, el texto no está normalizado, parte de las transformaciones pueden incluir realizar algún tipo de normalización. Para eso hagamos uso de [`gensim`](https://radimrehurek.com/gensim/index.html), en particular utilizaremos el módulo [`preprocessing`](https://radimrehurek.com/gensim/parsing/preprocessing.html#module-gensim.parsing.preprocessing) que se encargará de hacer varias normalizaciones por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ['reviewers', 'mentioned', 'watching', 'episode', 'hooked', 'right', 'exactly', 'happened', 'thing', 'struck', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'trust', 'faint', 'hearted', 'timid', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'called', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'city', 'home', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'far', 'away', 'main', 'appeal', 'fact', 'goes', 'shows', 'wouldn', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'mess', 'episode', 'saw', 'struck', 'nasty', 'surreal', 'couldn', 'ready', 'watched', 'developed', 'taste', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', 'sold', 'nickel', 'inmates', 'kill', 'order', 'away', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'comfortable', 'uncomfortable', 'viewing', 'thats', 'touch', 'darker'], 'target': 1}\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocess:\n",
    "    def __init__(self, filters=None):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        \n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _encode_target(self, target):\n",
    "        return 1 if target == \"positive\" else 0\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self._preprocess_string(item[\"data\"])\n",
    "        else:\n",
    "            data = [self._preprocess_string(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self._encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self._encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }\n",
    "\n",
    "preprocess = TextPreprocess()\n",
    "print(preprocess(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conversión a vectores\n",
    "\n",
    "Podemos continuar convertiendo el texto en una representación por vectores. Si bien hay muchas posibilidades (siendo la bolsa de palabras una de las más utilizadas), en general para Deep Learning se prefieren representaciones utilizando vectores contínuos, obtenidos por algún método del estilo de Word2Vec, Glove o FastText. Para este caso utilizaremos las representaciones de Glove de dimensión 50 que se dejaron para descargar en el [notebook 0](./0_set_up.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[-0.18105   , -0.79229999, -0.097616  , ...,  1.42859995,\n",
       "         -0.032471  ,  0.47235999],\n",
       "        [ 0.69395   ,  0.69261003, -0.21608   , ...,  0.2247    ,\n",
       "         -0.23197   ,  0.0062523 ],\n",
       "        [-0.0049087 ,  0.12611   ,  0.14056   , ..., -0.58464003,\n",
       "         -0.31830999,  0.31564   ],\n",
       "        ...,\n",
       "        [ 0.25435999, -0.44304001, -0.12524   , ...,  0.73352998,\n",
       "          0.026198  ,  0.30408001],\n",
       "        [-0.058468  ,  0.019087  ,  0.089056  , ..., -0.28176001,\n",
       "          0.045137  , -0.18802001],\n",
       "        [ 0.14443   ,  0.39103001, -0.93454999, ..., -0.71325999,\n",
       "         -0.54575998,  0.13952   ]]),\n",
       " 'target': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorizeText:\n",
    "    def __init__(self, glove_vectors_path):\n",
    "        with tempfile.NamedTemporaryFile(mode=\"w\") as tfh:\n",
    "            glove2word2vec(\"./data/glove.6B.50d.txt.gz\", tfh.name)\n",
    "            self.glove_model = KeyedVectors.load_word2vec_format(tfh.name)\n",
    "        self.unkown_vector = np.random.randn(self.glove_model.vector_size)  # Random vector for unknown words\n",
    "    \n",
    "    def _get_vector(self, word):\n",
    "        if word in self.glove_model:\n",
    "            return self.glove_model[word]\n",
    "        else:\n",
    "            return self.unkown_vector\n",
    "    \n",
    "    def _get_vectors(self, sentence):\n",
    "        return np.vstack([self._get_vector(word) for word in sentence])\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        review = []\n",
    "        if isinstance(item[\"data\"][0], str):\n",
    "            review = self._get_vectors(item[\"data\"])\n",
    "        else:\n",
    "            review = [self._get_vectors(d) for d in item[\"data\"]]\n",
    "\n",
    "        return {\n",
    "            \"data\": review,\n",
    "            \"target\": item[\"target\"]\n",
    "        }\n",
    "\n",
    "vectorizer = VectorizeText(\"./data/glove.6B.50d.txt.gz\")\n",
    "vectorizer(preprocess(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combinación de vectores\n",
    "\n",
    "Si bien ahora estamos con una versión de los atributos que podría pasar por una red neuronal, hay un problema, las distintas reviews tienen largo distinto y como el algoritmo se entrena en lotes (*mini-batches*) estas requieren tener todas el mismo largo. Hay varias maneras de lidiar con esto, cada una con sus ventajas y desventajas. Dado que por ahora solo vimos perceptrón multicapa, que espera algo de tamaño fijo, una opción sencilla puede ser la de simplemente promediar los vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectorsAverage:\n",
    "    def __call__(self, item):\n",
    "        if item[\"data\"][0].ndim == 2:\n",
    "            data = np.vstack([np.mean(d, axis=0) for d in item[\"data\"]])\n",
    "        else:\n",
    "            data = np.mean(item[\"data\"], axis=0)\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": item[\"target\"]\n",
    "        }\n",
    "\n",
    "vector_average = WordVectorsAverage()\n",
    "vector_average(vectorizer(preprocess(dataset[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conversión de vectores a tensores\n",
    "\n",
    "En el paso final, debemos convertir nuestros datos de arrays de `numpy` a tensores de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, item):\n",
    "        \"\"\"\n",
    "        This espects a single array.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"data\": torch.from_numpy(item[\"data\"]),\n",
    "            \"target\": torch.tensor(item[\"target\"])\n",
    "        }\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "to_tensor(vector_average(vectorizer(preprocess(dataset[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Componiendo las transformaciones\n",
    "\n",
    "Para evitar tener que llamar a todas las funciones de transformación que querramos aplicar, para ello hacemos uso del parámetro `transform` que definimos en nuestro `Dataset` y un poco de ayuda de `functools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compose(*functions):\n",
    "    return functools.reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n",
    "\n",
    "dataset = IMDBReviewsDataset(\"./data/imdb_reviews.csv.gz\",\n",
    "                             transform=compose(preprocess, vectorizer, vector_average, to_tensor))\n",
    "print(f\"Dataset loaded with {len(dataset)} elements\")\n",
    "print(f\"Sample element:\\n{dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Iterando el dataset\n",
    "\n",
    "Ya tenemos nuestro conjunto de datos con sus respectivas transformaciones. ¿Para qué nos sirve esto? Una opción es simplemente iterar en el conjunto de datos de a un elemento. Esto es sencillo, simplemente se hace a través de un `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(dataset):\n",
    "    print(sample[\"data\"])\n",
    "    print(sample[\"target\"])\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase Dataloader\n",
    "\n",
    "El problema con iterar de a un elemento es que estamos limitados al querer entrenar un modelo. Por empezar, los modelos de Deep Learning suelen ser más eficientes si se entrenan utilizando algún tipo de entrenamiento por *mini-batches*. Además, hay otras cosas como mezclar los elementos (*shuffling*) o cargar datos en paralelo vía distintos *multiprocess workers*. La clase [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) precisamente se encarga de hacer eso por nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, \n",
    "          sample_batched['data'].size(),\n",
    "          sample_batched['target'].size())\n",
    "\n",
    "    if i_batch == 2:\n",
    "        print(sample_batched[\"data\"])\n",
    "        print(sample_batched[\"target\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase IterableDataset\n",
    "\n",
    "El método preferido para trabajar con conjuntos de datos en PyTorch es `torch.utils.data.Dataset`. En general, hacer uso inteligente del método `__getitem__`, e.g. usándolo para cargar imágenes a medida que sean necesitadas y no al instanciar el dataset, es la mejor manera de trabajar con un conjunto de datos. En particular, de esta forma es mucho más fácil hacer *shuffling* de los datos y demás. No obstante, no siempre esto es posible, muchas veces el conjunto de datos es demasiado grande para levantarlo en memoria (aunque sólo levantemos referencias). Para esos casos, PyTorch ofrece la clase [`torch.utils.data.IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), en este caso el único método que es requerido implementar es `__iter__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(IterableDataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.dataset_path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        with gzip.open(self.dataset_path, \"rt\") as fh:\n",
    "            csv_reader = csv.reader(fh)\n",
    "            _ = next(csv_reader)  # Remove the header\n",
    "            for (_, _, title, category) in csv_reader:\n",
    "                item = {\n",
    "                    \"data\": title,\n",
    "                    \"target\": category\n",
    "                }\n",
    "                \n",
    "                if self.transform:\n",
    "                    yield self.transform(item)\n",
    "                else:\n",
    "                    yield item\n",
    "\n",
    "dataset = MeLiChallengeDataset(\"./data/meli-challenge-2019/spanish.train.csv.gz\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "dataiter = iter(dataloader)\n",
    "print(f\"Sample batch:\\n{dataiter.next()}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
