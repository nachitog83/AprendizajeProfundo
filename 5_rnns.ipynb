{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs para Martín Fierro\n",
    "\n",
    "El objetivo de los ejercicios en este tutorial es mostrar el impacto de algunas decisiones de diseño en la implementación de las redes neuronales, particularmente las recurrentes. Como ejemplo veremos una implementación de la red RNN para generación de lenguaje basada en caracteres de [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Para entrenarla utilizaremos un fragmento del Martín Fierro que pueden descargar [aquí](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt). Para un entrenamiento más complejo, pueden utilizar las obras completas de borges, disponibles en [este link](https://drive.google.com/file/d/0B4remi0ZCiqbUFpTS19pSmVFYkU/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import unicodedata\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s: %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leeremos el dataset del archivo de texto y lo preprocesaremos para disminuir la viariación de caracteres. Normalizaremos el formato unicos, elminaremos espacios y transformaremos todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('martin_fierro.txt'):\n",
    "    wget.download('https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, contaremos la cantidad de caracteres únicos presentes en el texto, y le asignaremos a cada uno un índice único y secuencial. Este índice será utilizado luego para crear las representaciones one-hot encoding de los caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 54\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print('Total chars: %d' % len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "La tarea de Language Modiling (**LM**) es aprender $P_{\\theta}$ parametrizada por $\\theta$ para determinar $P_{\\theta}(x|x_1,...x_t)$, donde $x$ puede ser caracter o palabra.\n",
    "\n",
    "$LM(x, x_1, ..., x_t) = P_{\\theta}(x | x_1,...x_t)$\n",
    "\n",
    "Para la tarea de generación, la tarea de **LM** es la ideal: dada una secuencia de entrada $x_1, ..., x_t$, podemos predecir la palabra de mayor probabilidad según nuestra probabilidad $P$.\n",
    "\n",
    "$GenerationLM(x_1, ..., x_t) = max_{x} P_{\\theta, x_1,...x_t}(x)$\n",
    "\n",
    "Otra opción, sería sortear la siguiente palabra con respecto a la distribución generada, es decir, samplear $x \\sim P_{\\theta, x_1,...x_t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Esqueleto de la red neuronal\n",
    "\n",
    "Lo primero que debemos pensar es cómo será la arquitectura de nuestra red para resolver la tarea deseada. En esta sección crearemos el modelo sequencial con PyTorch que representará nuestra red. En los pasos siguientes, implementaremos las transformaciones del corpus, por lo que en este paso pueden asumir cualquier formato en los datos de entrada.\n",
    "\n",
    "Para poder implementar el modelo debemos responder las siguientes preguntas:\n",
    "  - ¿Es una red one-to-one, one-to-many, many-to-one o many-to-many?\n",
    "  - ¿Cuál es el formato de entrada y de salida de la red? ¿Cuál es el tamaño de las matrices (tensores) de entrada y de salida?\n",
    "  - Luego de que la entrada pasa por la capa recurrente, ¿qué tamaño tiene el tensor?\n",
    "  - ¿Cómo se conecta la salida de la capa recurrente con la capa densa que realiza la clasificación?\n",
    "  - ¿Cuál es el loss apropiado para este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos los módulos que necesitaremos para implementar nuestra red:\n",
    "  - torch: acceso a todo el framework\n",
    "  - torch.nn: nos da acceso a capas ya implementadas y a la clase Module para instanciar y crear nuestra red\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if we have a GPU available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_layer,\n",
    "                 num_layers=1, dropout=0., bias=True,\n",
    "                 bidirectional=False):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        logging.info(f\"vocab_size: {vocab_size}\")\n",
    "        logging.info(f\"input_size: {input_size}\")\n",
    "        \n",
    "        # Set our LSTM parameters\n",
    "        self.lstm_config = {'input_size': input_size,\n",
    "                            'hidden_size': hidden_layer,\n",
    "                            'num_layers': num_layers,\n",
    "                            'bias': bias,\n",
    "                            'batch_first': True,\n",
    "                            'dropout': dropout,\n",
    "                            'bidirectional': bidirectional}\n",
    "        \n",
    "        # Set our FC layer parameters\n",
    "        self.linear_config = {'in_features': hidden_layer,\n",
    "                              'out_features': vocab_size,\n",
    "                              'bias': bias}\n",
    "        \n",
    "        # Instanciate the layers\n",
    "        self.encoder = nn.LSTM(**self.lstm_config)\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('linear', nn.Linear(**self.linear_config))\n",
    "        self.decoder.add_module('softmax',nn.LogSoftmax(dim=-1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs, _ = self.encoder(inputs)\n",
    "        predictions = self.decoder(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 10:00:51,663: INFO - vocab_size: 54\n",
      "2021-03-20 10:00:51,664: INFO - input_size: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (encoder): LSTM(54, 128, batch_first=True)\n",
      "  (decoder): Sequential(\n",
      "    (linear): Linear(in_features=128, out_features=54, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(len(chars), len(chars), 128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Transformación del input\n",
    "\n",
    "Una vez que definimos la arquitectura de la red, sabemos con exactitud cuál es el input que necesitamos utilizar. En esta sección transformaremos el texto que leimos del archivo en ejemplos de entrenamiento para nuestra red. El resultado será una matrix que representa las secuencias de caracteres y una matriz que representa las etiquetas correspondientes.\n",
    "\n",
    "  - ¿Cómo debemos representar cada ejemplo?\n",
    "  - ¿Cómo debemos representar cada etiqueta?\n",
    "  \n",
    "Usaremos la clase torch.Dataset para implementar nuestro. Necesitamos implementar dos métodos para esto:\n",
    "  - \\__len__: retorna el largo del dataset\n",
    "  - \\__getitem__: dado un id, retorna el elemento asociado en el dataset con ese id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MartinFierroDataset(Dataset):\n",
    "    def __init__(self, textdata, maxlen):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for i in range(0, len(textdata) - maxlen - 1, maxlen):\n",
    "            sentences.append(textdata[i: i + maxlen])\n",
    "            next_chars.append(textdata[i + 1: i + maxlen + 1])\n",
    "        self.length = len(sentences)\n",
    "\n",
    "        self.X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "        self.y = np.zeros((len(sentences), maxlen), dtype=np.float32)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                self.X[i, t, char_indices[char]] = 1\n",
    "                self.y[i, t] = char_indices[next_chars[i][t]]\n",
    "        \n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        output = {'X': self.X[idx],\n",
    "                  'y': self.y[idx]}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB sequences: 677\n"
     ]
    }
   ],
   "source": [
    "data = MartinFierroDataset(text, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((677, 50, 54), (677, 50))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.X.shape, data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 28.,  0., ..., 53., 24., 30.],\n",
       "       [20.,  2.,  0., ..., 36., 23., 28.],\n",
       "       [32., 20., 36., ...,  0., 22., 33.],\n",
       "       ...,\n",
       "       [20.,  0., 30., ..., 41., 33.,  0.],\n",
       "       [38., 24., 32., ...,  0., 23., 24.],\n",
       "       [ 0., 37., 20., ..., 20., 32., 38.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenamiento de la red\n",
    "\n",
    "En esta sección entrenaremos nuestra red. Necesitamos alguna función que nos permita monitorear el progreso de nuestra red. Para eso vamos a imprimir una muestra del texto generado por la red cada cierta cantidad de epochs.\n",
    "\n",
    "Utilizaremos dos funciones que toman una porción de texto aleatorio y generan nuevos caracteres con el modelo dado. \n",
    "\n",
    "    - ¿Cómo podemos interpretar la salida de la red? ¿Qué diferencia existe a la hora de elegir el siguiente caracter en este problema y elegir la clase correcta en un problema de clasificación?\n",
    "    - ¿Qué hacen estas funciones? ¿Para qué se utiliza la variable diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\\n\"\n",
    "    temp_preds = np.asarray(preds[:,-1,:]).astype('float64') / temperature\n",
    "    exp_preds = np.exp(temp_preds)\n",
    "    new_probs = (exp_preds / np.sum(exp_preds)).squeeze()\n",
    "    probas = np.random.multinomial(1, new_probs, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def print_sample(model, device, maxlen=50):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_size = 200\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            #sentence = 'el bien perdido'\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(sentence)\n",
    "\n",
    "            # Printing the sample\n",
    "            for i in range(sample_size):\n",
    "                x = np.zeros((1, maxlen, len(chars)), dtype=np.float32)\n",
    "                # Build the one-hot encoding for the sentence\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                input_tensor = torch.tensor(x).to(device)\n",
    "                    \n",
    "                logprob_preds = model(input_tensor)\n",
    "                next_index = temperature_sample(logprob_preds.cpu().numpy(), diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Primero configuramos los hiperparámetros de la red. En este momento determinamos lo siguiente:\n",
    "  -  learning_rate\n",
    "  -  epochs\n",
    "  -  función de pérdida\n",
    "  -  optimizador\n",
    "  \n",
    "También definimos los parámetros para torch.DataLoader, clase que implementa un manejador del dataset que nos dividirá los datos en batches (y los distribuirá entre distintos nodos de cómputo, en caso de contar con multi GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (encoder): LSTM(54, 128, batch_first=True)\n",
       "  (decoder): Sequential(\n",
       "    (linear): Linear(in_features=128, out_features=54, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 22\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "dataloader_config = {'dataset': data,\n",
    "                     'batch_size': 32,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "# Send the model to GPU if there is one available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Time taken 3.18, Training-Loss 3.68409\n",
      "Epoch 002, Time taken 1.04, Training-Loss 3.10545\n",
      "Epoch 003, Time taken 0.97, Training-Loss 3.05537\n",
      "Epoch 004, Time taken 0.90, Training-Loss 3.03387\n",
      "Epoch 005, Time taken 1.55, Training-Loss 3.00906\n",
      "Epoch 006, Time taken 1.65, Training-Loss 2.95954\n",
      "Epoch 007, Time taken 0.91, Training-Loss 2.90459\n",
      "Epoch 008, Time taken 1.03, Training-Loss 2.82526\n",
      "Epoch 009, Time taken 1.48, Training-Loss 2.74348\n",
      "Epoch 010, Time taken 0.99, Training-Loss 2.67840\n",
      "Epoch 011, Time taken 0.91, Training-Loss 2.60960\n",
      "Epoch 012, Time taken 0.91, Training-Loss 2.55150\n",
      "Epoch 013, Time taken 1.09, Training-Loss 2.50417\n",
      "Epoch 014, Time taken 1.01, Training-Loss 2.45799\n",
      "Epoch 015, Time taken 1.02, Training-Loss 2.41406\n",
      "Epoch 016, Time taken 1.09, Training-Loss 2.38923\n",
      "Epoch 017, Time taken 1.46, Training-Loss 2.34813\n",
      "Epoch 018, Time taken 1.42, Training-Loss 2.32721\n",
      "Epoch 019, Time taken 0.91, Training-Loss 2.30095\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" todo, lo amarran codo con codo 275 y pa el cepo l\"\n",
      " todo, lo amarran codo con codo 275 y pa el cepo la tento e el sa la en en la de la la paras en en me me de en la de mere en en an al el can an ento en enta de la la al are paro en el en anta da en pera an la an an an el para de para la me en ala al \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" todo, lo amarran codo con codo 275 y pa el cepo l\"\n",
      " todo, lo amarran codo con codo 275 y pa el cepo le caranos la la tos ancano le an lo danta da el carde tos sede ha liran al lasta lo en he cen al an da dos paro me puar el ma tarta que la do las anmirro el alen un erta al cos la cas guedas en cen a \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" todo, lo amarran codo con codo 275 y pa el cepo l\"\n",
      " todo, lo amarran codo con codo 275 y pa el cepo livirane , ome nmedio mecas epos el licór calo oi mundo cora motminumó tíbio vala. 3i0 alan os lúncorivr raneo  ué sus  aus pena rfason que ongal lamemaay en al libpazto. 5 y pondo 9e ae eqheb do lo de\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" todo, lo amarran codo con codo 275 y pa el cepo l\"\n",
      " todo, lo amarran codo con codo 275 y pa el cepo lamtas so piron! qumemlo e, que her y ledno as cono l hóques dndiste  7p4tlghans fuuo. :, gus7ciel, filhr nhen bvee mé e amoltni ii e eleds hoás rrda íéz la  mes, que, rernda pirienoldejro fim en óns e\n",
      "Epoch 020, Time taken 6.09, Training-Loss 2.28308\n",
      "Epoch 021, Time taken 1.06, Training-Loss 2.26396\n",
      "Epoch 022, Time taken 1.08, Training-Loss 2.24575\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "historical_loss = torch.FloatTensor()\n",
    "\n",
    "# Set the model on train mode\n",
    "model.train()\n",
    "for epoch in range(1,epochs+1):\n",
    "    loss = 0\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # Show samples every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print_sample(model, device)\n",
    "        model.train()\n",
    "\n",
    "    train_loss = torch.FloatTensor().to(device)\n",
    "        \n",
    "    dataloader = DataLoader(**dataloader_config)\n",
    "    for i_batch, sample in enumerate(dataloader):\n",
    "        inputs, gt_out = sample['X'].to(device), sample['y'].to(device)\n",
    "        preds = model(inputs)\n",
    "        bs, seqlen, cat = preds.size()\n",
    "\n",
    "        # preds: batch_size x max_seq_length x len(chars)\n",
    "        # gt_out: batch_size x max_seq_length\n",
    "        # NLLLoss expects inputs of the form:\n",
    "        # N x C x d1 x ... x dt for the input\n",
    "        # N x d1 x ... x dt for the targets\n",
    "        # So we transform define N = batch_size x max_seq_length\n",
    "        # and we keep C = len(chars)\n",
    "        loss = loss_function(preds.view(bs*seqlen, -1),\n",
    "                             gt_out.view(bs*seqlen).type(torch.long)).unsqueeze(0)\n",
    "        \n",
    "        # Set gradients to 0, backpropagate, make an uptimization\n",
    "        # update and store the loss for logging purposes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = torch.cat([train_loss, loss])      \n",
    "        \n",
    "    print(\"Epoch %03d, Time taken %.2f, Training-Loss %.5f\" % (epoch, time()-start, torch.mean(train_loss)))\n",
    "    with torch.no_grad():\n",
    "        historical_loss = torch.cat([historical_loss, torch.mean(train_loss.cpu()).view(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios extras\n",
    "\n",
    "Una vez que hemos implementado la arquitectura básica de la red, podemos comenzar a experimentar con distintas modificaciones para lograr mejores resultados. Algunas tareas posibles son:\n",
    "\n",
    " - Agregar más capas recurrentes\n",
    " - Probar otros largos de secuencias máximas\n",
    " - Agregar capas de regularización y/o dropout\n",
    " - Agregar métricas de performance como perplexity y word error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones\n",
    "\n",
    "Para asegurarnos de que el modelo esté efectivamente entrenando, podemos graficar la función de pérdida en el corpus de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef2b97b89e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "loss_values = historical_loss.detach().numpy()\n",
    "seaborn.lineplot(x=range(loss_values.shape[0]), y=loss_values)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
